{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1302a608-4b4d-46bf-bd0c-b4f13eff2e5e",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_class_name: hidden\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3571cc",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/data_generation.ipynb)\n",
    "\n",
    "# Generate Synthetic Data\n",
    "\n",
    "Synthetic data is artificially generated data, rather than data collected from real-world events. It's used to simulate real data without compromising privacy or encountering real-world limitations. \n",
    "\n",
    "Benefits of Synthetic Data:\n",
    "\n",
    "1. **Privacy and Security**: No real personal data at risk of breaches.\n",
    "2. **Data Augmentation**: Expands datasets for machine learning.\n",
    "3. **Flexibility**: Create specific or rare scenarios.\n",
    "4. **Cost-effective**: Often cheaper than real-world data collection.\n",
    "5. **Regulatory Compliance**: Helps navigate strict data protection laws.\n",
    "6. **Model Robustness**: Can lead to better generalizing AI models.\n",
    "7. **Rapid Prototyping**: Enables quick testing without real data.\n",
    "8. **Controlled Experimentation**: Simulate specific conditions.\n",
    "9. **Access to Data**: Alternative when real data isn't available.\n",
    "\n",
    "Note: Despite the benefits, synthetic data should be used carefully, as it may not always capture real-world complexities.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "In this notebook, we'll dive deep into generating synthetic medical billing records using the `langchain` library. This tool is particularly useful when you want to develop or test algorithms but don't want to use real patient data due to privacy concerns or data availability issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca57012",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, you'll need to have the `langchain` library installed, along with its dependencies. Since we're using the OpenAI generator chain, we'll install that as well. Since this is an experimental library, we'll need to include `langchain_experimental` in our installation. We'll then import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0377478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-chroma 0.1.3 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.6 which is incompatible.\n",
      "embedchain 0.1.102 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.1 which is incompatible.\n",
      "embedchain 0.1.102 requires langchain-openai<0.0.6,>=0.0.5, but you have langchain-openai 0.2.1 which is incompatible.\n",
      "embedchain 0.1.102 requires pypdf<4.0.0,>=3.11.0, but you have pypdf 4.3.0 which is incompatible.\n",
      "embedchain 0.1.102 requires tiktoken<0.6.0,>=0.5.2, but you have tiktoken 0.7.0 which is incompatible.\n",
      "langchain-cohere 0.1.9 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.3.6 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain<0.3,>=0.2, but you have langchain 0.3.1 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.3.1 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain-openai<0.2,>=0.1, but you have langchain-openai 0.2.1 which is incompatible.\n",
      "crewai-tools 0.0.6 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.1 which is incompatible.\n",
      "langchain-together 0.1.4 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-together 0.1.4 requires langchain-openai<0.2.0,>=0.1.8, but you have langchain-openai 0.2.1 which is incompatible.\n",
      "langchain-anthropic 0.1.20 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.6 which is incompatible.\n",
      "crewai 0.14.3 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.3.1 which is incompatible.\n",
      "crewai 0.14.3 requires langchain-openai<0.0.6,>=0.0.5, but you have langchain-openai 0.2.1 which is incompatible.\n",
      "langchain-google-vertexai 1.0.6 requires langchain-core<0.3,>=0.2.9, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-fireworks 0.1.5 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-mistralai 0.1.10 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-qdrant 0.1.2 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.6 which is incompatible.\n",
      "prompt-generator 0.1.0 requires langchain<0.2.0,>=0.1.16, but you have langchain 0.3.1 which is incompatible.\n",
      "prompt-generator 0.1.0 requires langchain-community<0.0.33,>=0.0.32, but you have langchain-community 0.3.1 which is incompatible.\n",
      "prompt-generator 0.1.0 requires langchain-openai<0.2.0,>=0.1.3, but you have langchain-openai 0.2.1 which is incompatible.\n",
      "prompt-generator 0.1.0 requires tiktoken<0.7.0,>=0.6.0, but you have tiktoken 0.7.0 which is incompatible.\n",
      "langchain-aws 0.1.11 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-google-genai 1.0.7 requires langchain-core<0.3,>=0.2.9, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langchain-groq 0.1.6 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.3.6 which is incompatible.\n",
      "langgraph 0.1.8 requires langchain-core<0.3,>=0.2.15, but you have langchain-core 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain_experimental langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391ae036-36a2-4d2a-8e1b-5a536b268b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b47bb31-b4c7-42ff-9253-2c94747db750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_experimental.tabular_synthetic_data.openai import (\n",
    "    OPENAI_TEMPLATE,\n",
    "    create_openai_data_generator,\n",
    ")\n",
    "from langchain_experimental.tabular_synthetic_data.prompts import (\n",
    "    SYNTHETIC_FEW_SHOT_PREFIX,\n",
    "    SYNTHETIC_FEW_SHOT_SUFFIX,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0917b",
   "metadata": {},
   "source": [
    "## 1. Define Your Data Model\n",
    "Every dataset has a structure or a \"schema\". The `MedicalBilling` class below serves as our schema for the synthetic data. By defining this, we're informing our synthetic data generator about the shape and nature of data we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291bad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalBilling(BaseModel):\n",
    "    patient_id: int\n",
    "    patient_name: str\n",
    "    diagnosis_code: str\n",
    "    procedure_code: str\n",
    "    total_charge: float\n",
    "    insurance_claim_amount: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059ca63",
   "metadata": {},
   "source": [
    "For instance, every record will have a `patient_id` that's an integer, a `patient_name` that's a string, and so on.\n",
    "\n",
    "## 2. Sample Data\n",
    "To guide the synthetic data generator, it's useful to provide it with a few real-world-like examples. These examples serve as a \"seed\" - they're representative of the kind of data you want, and the generator will use them to create  data that looks similar to your expectations.\n",
    "\n",
    "Here are some fictional medical billing records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b989b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"example\": \"\"\"Patient ID: 123456, Patient Name: John Doe, Diagnosis Code: \n",
    "        J20.9, Procedure Code: 99203, Total Charge: $500, Insurance Claim Amount: $350\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"\"\"Patient ID: 789012, Patient Name: Johnson Smith, Diagnosis \n",
    "        Code: M54.5, Procedure Code: 99213, Total Charge: $150, Insurance Claim Amount: $120\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"\"\"Patient ID: 345678, Patient Name: Emily Stone, Diagnosis Code: \n",
    "        E11.9, Procedure Code: 99214, Total Charge: $300, Insurance Claim Amount: $250\"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e28809",
   "metadata": {},
   "source": [
    "## 3. Craft a Prompt Template\n",
    "The generator doesn't magically know how to create our data; we need to guide it. We do this by creating a prompt template. This template helps instruct the underlying language model on how to produce synthetic data in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6e042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNTHETIC_FEW_SHOT_PREFIX: This is a test about generating synthetic data about {subject}. Examples below:\n",
      "SYNTHETIC_FEW_SHOT_SUFFIX: Now you generate synthetic data about {subject}. Make sure to {extra}:\n"
     ]
    }
   ],
   "source": [
    "OPENAI_TEMPLATE = PromptTemplate(input_variables=[\"example\"], template=\"{example}\")\n",
    "\n",
    "print('SYNTHETIC_FEW_SHOT_PREFIX:', SYNTHETIC_FEW_SHOT_PREFIX)\n",
    "print('SYNTHETIC_FEW_SHOT_SUFFIX:', SYNTHETIC_FEW_SHOT_SUFFIX)\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    prefix=SYNTHETIC_FEW_SHOT_PREFIX,\n",
    "    examples=examples,\n",
    "    suffix=SYNTHETIC_FEW_SHOT_SUFFIX,\n",
    "    input_variables=[\"subject\", \"extra\"],\n",
    "    example_prompt=OPENAI_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6da3cb",
   "metadata": {},
   "source": [
    "The `FewShotPromptTemplate` includes:\n",
    "\n",
    "- `prefix` and `suffix`: These likely contain guiding context or instructions.\n",
    "- `examples`: The sample data we defined earlier.\n",
    "- `input_variables`: These variables (\"subject\", \"extra\") are placeholders you can dynamically fill later. For instance, \"subject\" might be filled with \"medical_billing\" to guide the model further.\n",
    "- `example_prompt`: This prompt template is the format we want each example row to take in our prompt.\n",
    "\n",
    "## 4. Creating the Data Generator\n",
    "With the schema and the prompt ready, the next step is to create the data generator. This object knows how to communicate with the underlying language model to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9ba911",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_generator = create_openai_data_generator(\n",
    "    output_schema=MedicalBilling,\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=1\n",
    "    ),  # You'll need to replace with your actual Language Model instance\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4198bd6",
   "metadata": {},
   "source": [
    "## 5. Generate Synthetic Data\n",
    "Finally, let's generate our synthetic data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a424c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_results = synthetic_data_generator.generate(\n",
    "    subject=\"medical_billing\",\n",
    "    extra=\"the name must be chosen at random. Make it something you wouldn't normally choose.\",\n",
    "    runs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4402e9",
   "metadata": {},
   "source": [
    "This command asks the generator to produce 10 synthetic medical billing records. The results are stored in `synthetic_results`. The output will be a list of the `MedicalBilling` pydantic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4cbf9",
   "metadata": {},
   "source": [
    "### Other implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e715d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.synthetic_data import (\n",
    "    DatasetGenerator,\n",
    "    create_data_generation_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fccedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "chain = create_data_generation_chain(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4314c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12052/1337896113.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain({\"fields\": [\"blue\", \"yellow\"], \"preferences\": {}})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fields': ['blue', 'yellow'],\n",
       " 'preferences': {},\n",
       " 'text': 'The vibrant blue sky was dotted with fluffy yellow clouds, creating a stunning contrast against the backdrop of the rolling green hills.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"fields\": [\"blue\", \"yellow\"], \"preferences\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b116c487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': {'colors': ['blue', 'yellow']},\n",
       " 'preferences': {'style': 'Make it in a style of a weather forecast.'},\n",
       " 'text': \"In today's weather forecast, we can expect a beautiful display of blue skies with scattered patches of bright yellow sunshine throughout the day.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\n",
    "        \"fields\": {\"colors\": [\"blue\", \"yellow\"]},\n",
    "        \"preferences\": {\"style\": \"Make it in a style of a weather forecast.\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff823394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': {'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']},\n",
       " 'preferences': None,\n",
       " 'text': 'Tom Hanks, known for his incredible performances in movies such as \"Forrest Gump\" and \"Green Mile,\" has captivated audiences worldwide with his versatility and talent.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\n",
    "        \"fields\": {\"actor\": \"Tom Hanks\", \"movies\": [\"Forrest Gump\", \"Green Mile\"]},\n",
    "        \"preferences\": None,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea1ad5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']},\n",
       "  {'actor': 'Mads Mikkelsen', 'movies': ['Hannibal', 'Another round']}],\n",
       " 'preferences': {'minimum_length': 200, 'style': 'gossip'},\n",
       " 'text': 'Did you hear that Tom Hanks, known for his iconic roles in movies like Forrest Gump and Green Mile, has been spotted out dining with Mads Mikkelsen, the talented actor from Hannibal and Another round? It seems like these two Hollywood stars are forming a new friendship off-screen!'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\n",
    "        \"fields\": [\n",
    "            {\"actor\": \"Tom Hanks\", \"movies\": [\"Forrest Gump\", \"Green Mile\"]},\n",
    "            {\"actor\": \"Mads Mikkelsen\", \"movies\": [\"Hannibal\", \"Another round\"]},\n",
    "        ],\n",
    "        \"preferences\": {\"minimum_length\": 200, \"style\": \"gossip\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7a4bb",
   "metadata": {},
   "source": [
    "As we can see, the created examples are diversified and possess information we wanted them to have. Also, their style reflects our given preferences quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7f55a",
   "metadata": {},
   "source": [
    "## Generating exemplary dataset for extraction benchmarking purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e98bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [\n",
    "    {\n",
    "        \"Actor\": \"Tom Hanks\",\n",
    "        \"Film\": [\n",
    "            \"Forrest Gump\",\n",
    "            \"Saving Private Ryan\",\n",
    "            \"The Green Mile\",\n",
    "            \"Toy Story\",\n",
    "            \"Catch Me If You Can\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"Actor\": \"Tom Hardy\",\n",
    "        \"Film\": [\n",
    "            \"Inception\",\n",
    "            \"The Dark Knight Rises\",\n",
    "            \"Mad Max: Fury Road\",\n",
    "            \"The Revenant\",\n",
    "            \"Dunkirk\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "generator = DatasetGenerator(model, {\"style\": \"informal\", \"minimal length\": 500})\n",
    "dataset = generator(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478eaca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fields': {'Actor': 'Tom Hanks',\n",
       "   'Film': ['Forrest Gump',\n",
       "    'Saving Private Ryan',\n",
       "    'The Green Mile',\n",
       "    'Toy Story',\n",
       "    'Catch Me If You Can']},\n",
       "  'preferences': {'style': 'informal', 'minimal length': 500},\n",
       "  'text': 'Tom Hanks, known for his iconic roles in films such as \"Forrest Gump,\" \"Saving Private Ryan,\" \"The Green Mile,\" \"Toy Story,\" and \"Catch Me If You Can,\" has solidified himself as one of the most versatile and beloved actors in Hollywood.'},\n",
       " {'fields': {'Actor': 'Tom Hardy',\n",
       "   'Film': ['Inception',\n",
       "    'The Dark Knight Rises',\n",
       "    'Mad Max: Fury Road',\n",
       "    'The Revenant',\n",
       "    'Dunkirk']},\n",
       "  'preferences': {'style': 'informal', 'minimal length': 500},\n",
       "  'text': 'Tom Hardy, known for his roles in films such as \"Inception,\" \"The Dark Knight Rises,\" \"Mad Max: Fury Road,\" \"The Revenant,\" and \"Dunkirk,\" has captivated audiences worldwide with his intense performances and versatility on the big screen.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a7d64",
   "metadata": {},
   "source": [
    "## Extraction from generated examples\n",
    "Okay, let's see if we can now extract output from this generated data and how it compares with our case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c6a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9461d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    Actor: str = Field(description=\"name of an actor\")\n",
    "    Film: List[str] = Field(description=\"list of names of films they starred in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390171d",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5528d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12052/1937598790.py:11: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = llm(_input.to_string())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Actor(Actor='Tom Hanks', Film=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Toy Story', 'Catch Me If You Can'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract fields from a given text.\\n{format_instructions}\\n{text}\\n\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(text=dataset[0][\"text\"])\n",
    "output = llm(_input.to_string())\n",
    "\n",
    "parsed = parser.parse(output)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926a7eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(parsed.Actor == inp[0][\"Actor\"]) & (parsed.Film == inp[0][\"Film\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f0b87",
   "metadata": {},
   "source": [
    "### Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523bb584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12052/4290255188.py:1: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extractor = create_extraction_chain_pydantic(pydantic_schema=Actor, llm=model)\n",
      "/tmp/ipykernel_12052/4290255188.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  extracted = extractor.run(dataset[1][\"text\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Actor(Actor='Tom Hardy', Film=['Inception', 'The Dark Knight Rises', 'Mad Max: Fury Road', 'The Revenant', 'Dunkirk'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = create_extraction_chain_pydantic(pydantic_schema=Actor, llm=model)\n",
    "extracted = extractor.run(dataset[1][\"text\"])\n",
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8451c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(extracted[0].Actor == inp[1][\"Actor\"]) & (extracted[0].Film == inp[1][\"Film\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03de4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
