{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674a0d41-e3e3-4423-a995-25d40128c518",
   "metadata": {},
   "source": [
    "# How to do question answering over CSVs\n",
    "\n",
    "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:\n",
    "\n",
    "* **RECOMMENDED**: Load the CSV(s) into a SQL database, and use the approaches outlined in the [SQL tutorial](/docs/tutorials/sql_qa).\n",
    "* Give the LLM access to a Python environment where it can use libraries like Pandas to interact with the data.\n",
    "\n",
    "We will cover both approaches in this guide.\n",
    "\n",
    "## ⚠️ Security note ⚠️\n",
    "\n",
    "Both approaches mentioned above carry significant risks. Using SQL requires executing model-generated SQL queries. Using a library like Pandas requires letting the model execute Python code. Since it is easier to tightly scope SQL connection permissions and sanitize SQL queries than it is to sandbox Python environments, **we HIGHLY recommend interacting with CSV data via SQL.** For more on general security best practices, [see here](/docs/security)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c20d7-71e1-4808-9012-48278f3a9b94",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Dependencies for this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3fcf245-b0aa-4aee-8f0a-9c9cf94b065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-cohere 0.1.9 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.3.9 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain<0.3,>=0.2, but you have langchain 0.3.2 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.3.1 which is incompatible.\n",
      "gpt-researcher 0.8.0 requires langchain-openai<0.2,>=0.1, but you have langchain-openai 0.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e34a3-0978-4856-8844-d8dfc6d5ac51",
   "metadata": {},
   "source": [
    "Set required environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53913d79-4a11-4bc6-bb49-dea2cc8c453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LangSmith is recommended but not required. Uncomment below lines to use.\n",
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b4232-2f6a-4eb5-b0cb-1d48a9e02fcc",
   "metadata": {},
   "source": [
    "Download the [Titanic dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset) if you don't already have it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9099c7-5247-4edb-ba5d-10c3c4c60db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-08 11:33:54--  https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\n",
      "Resolving web.stanford.edu (web.stanford.edu)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.67.215.200, 2607:f6d0:0:925a::ab43:d7c8\n",
      "Connecting to web.stanford.edu (web.stanford.edu)|171.67.215.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44225 (43K) [text/csv]\n",
      "Saving to: ‘titanic.csv’\n",
      "\n",
      "titanic.csv         100%[===================>]  43.19K   205KB/s    in 0.2s    \n",
      "\n",
      "2024-10-08 11:33:55 (205 KB/s) - ‘titanic.csv’ saved [44225/44225]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv -O titanic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad029641-6d6c-44cc-b16f-2d5472672adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 8)\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779ab07-b715-49e5-ab2a-2e6be7d02927",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "Using SQL to interact with CSV data is the recommended approach because it is easier to limit permissions and sanitize queries than with arbitrary Python.\n",
    "\n",
    "Most SQL databases make it easy to load a CSV file in as a table ([DuckDB](https://duckdb.org/docs/data/csv/overview.html), [SQLite](https://www.sqlite.org/csv.html), etc.). Once you've done this you can use all of the chain and agent-creating techniques outlined in the [SQL tutorial](/docs/tutorials/sql_qa). Here's a quick example of how we might do this with SQLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61e9886-4713-4c88-87d4-dab439687f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_USER = \"wsl_user\"\n",
    "DB_PASSWORD = \"a124567\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"media_crawler\"\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "# df.to_sql(\"titanic\", engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3275fc91-3777-4f78-8edf-d148001684b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql\n",
      "['XhsNotes', 'bilibili_video', 'bilibili_video_comment', 'douyin_aweme', 'douyin_aweme_comment', 'dy_creator', 'kuaishou_video', 'kuaishou_video_comment', 'weibo_note', 'weibo_note_comment', 'xhs_creator', 'xhs_note', 'xhs_note_comment']\n",
      "[(1, '59532abe5e87e746a3d3e49f', '酒酿宝', 'https://sns-avatar-qc.xhscdn.com/avatar/6640b4ec239fb0f0db0b2af4.jpg', '中国香港', 1720058839710, 1720059297964, '6685fed1000000000a0264bc', 'normal', '🇭🇰新冠首阳 五天转阴', '躲过了前两年的新冠高峰期，没想到去完高雄旅游完回来居然中招了，果然新冠不会漏掉每一个人🤦🏻\\u200d♀️\\n\\t\\n说说我这五天的一个症状吧：\\n第一天6/27: 起床之后全身乏力，头痛炸裂，以为是旅游回来累的，晚上量体温37.7度\\n第二天 6/28: 早上起来量体温37.1度，浑身上下没力气，呼吸困难，干咳，胃口很好（怀疑自己是不是得的传说中的干饭株/猪瘟？😂）晚上一个人咔咔炫掉大半个pizza，检测呈浅阳，下午已经退烧，体温36.6度，晚上开始有点鼻塞，流一点点清水鼻涕，类似于加强版的鼻炎\\n第三天 6/29: 起床之后左边喉咙吞口水有一点点痛，持续鼻塞流鼻水，干咳，检测呈强阳\\n第四天 6/30:...', '', 1720057553000, 1720058733000, '2', '0', '2', '0', 'https://sns-img-al.xhscdn.com/1040g2sg314qhj6f4hc7049kvgqlbtp4vbv8q408,https://sns-img-al.xhscdn.com/3be80f2e-1df4-073d-f011-82accfb3b8d0,https://sns-img-al.xhscdn.com/d284980b-27cc-5c80-6665-5ca93521c0cc', '新冠,抗疫日记,香港居家隔离,covid19', 'https://www.xiaohongshu.com/explore/6685fed1000000000a0264bc', 1, 1, 1, 1, 1, 1, ''), (2, '62fe31ce000000000f005d1b', 'aquarius重生版', 'https://sns-avatar-qc.xhscdn.com/avatar/1040g2jo30vfkq54hm26g5onu6773on8r22tcfn8', '江苏', 1720058839717, 1720059297970, '6686035b000000000a024e84', 'normal', 'SA55究竟什么情况？', '有没有大神了解的？就这么无了？[惊恐R][惊恐R][惊恐R][哭惹R][哭惹R][哭惹R]\\n#新冠 #抗体 #新冠抗体', '', 1720058715000, 1720058715000, '0', '0', '3', '0', 'https://sns-img-qc.xhscdn.com/d02653aa-0635-d5c0-6abb-334ecb3e0e8d', '新冠,抗体,新冠抗体', 'https://www.xiaohongshu.com/explore/6686035b000000000a024e84', 1, 1, 0, 1, 1, 1, ''), (3, '6667007500000000030313db', '没有作业的星球', 'https://sns-avatar-qc.xhscdn.com/avatar/1040g2jo314oqjb21h4005pj701qgu4urc9md17o', '美国', 1720058839723, 1720059297958, '66860307000000001e011928', 'normal', '数据分析新冠对美国人出行的影响', '2022年计算社会科学国际会议 (IC2S2)...', '', 1720058631000, 1720058889000, '1', '0', '0', '0', 'https://sns-img-hw.xhscdn.com/07f4a5b2-e106-8405-d5f5-c53e0eba7565,https://sns-img-hw.xhscdn.com/f2d6fe48-51d3-686c-4504-e046c25421d3', '计算社会科学,新冠,疫情,旅游,美国,数据分析', 'https://www.xiaohongshu.com/explore/66860307000000001e011928', 1, 1, 0, 1, 0, 0, ''), (4, '59c07a70b1da141e143bcc92', '很哇塞', 'https://sns-avatar-qc.xhscdn.com/avatar/59c07a70b1da141e143bcc92.jpg', '', 1720058839729, 1725000689609, '668600e40000000003024f4e', 'normal', '在日本阳了，感冒了吃什么药！', '#新冠[话题]# #日本生病[话题]# #日本药妆店[话题]#\\n[猪头R]最近阳了，一周刚痊愈，自己带的国内的药都过期了，想把自己吃日本药痊愈的经验介绍给大家。\\n\\t\\n1，新冠检测不用去医院，亚马逊买试剂盒隔日就到，可以检测是病毒流感还是新冠\\n2.[鼓掌R][鼓掌R]图3的退烧止痛药！特别好用[点赞R]，吃下大概半小时会大量出汗，然后烧就退了很多，一天三次一次两片。[向右R]低烧就用图7退热贴物理降温！\\n\\t\\n3.嗓子非常难受...', '', 1720058084000, 1720058301000, '487', '912', '45', '279', 'http://sns-webpic-qc.xhscdn.com/202408301451/4984cc0ac633794051d1870c89c5c1cc/1040g008314qhr5jeh40049qeb9t71j4i72l4570!nd_dft_wlteh_webp_3,http://sns-webpic-qc.xhscdn.com/202408301451/339e2015e52a774d878651fcf484a729/1040g008314qhr5jeh40g49qeb9t71j4ibj1enu8!nd_dft_wlteh_webp_3,http://sns-webpic-q...', '新冠,日本生病,日本药妆店', 'https://www.xiaohongshu.com/explore/668600e40000000003024f4e?xsec_token=AB30e7EEjF8yniPpK6P1rYg4SwH9Anb7FBACtzvZQAkIk=&xsec_source=pc_search', 1, 1, 1, 1, 1, 1, ''), (5, '632b18e00000000023025a97', '瘦到100以下我就满足了', 'https://sns-avatar-qc.xhscdn.com/avatar/1040g2jo3139iouvig8605opb33g8smkns8llj7g', '湖北', 1720058839733, 1720059297979, '6686018b000000001c026d4f', 'normal', '没人告诉我治疗新冠的中药这么难喝', '二阳了，喝药真的是酷刑。\\n原本以为蒲地蓝口服液就够难喝了，又给开了金叶败毒颗粒，但是捏着鼻子也能灌下去。\\n结果！！医生说这个才是对症的中药，喊我第二天来中药房拿药，我欢天喜地拿回了家。\\n冲药的时候隐隐就觉得味道不太对劲，又香又臭的。\\n半个小时后药凉了，秉着猎奇的态度没有捏鼻子浅尝了一口，真的就是一瞬间的生理反应，哇的一口吐了出来。\\n又香又甜又苦又辣又辛，还很咸！\\n主要是它还有回味一直往上涌！\\n并且那股辛味儿顺着我咳了两天的嗓子往下灼烧，一瞬间感觉奇经八脉都打通了！！！\\n然后就是一阵一阵的回味在嘴里经久不消，我对象觉得很神奇，于是他也尝了一口，果不其然，脸直接皱成了一团……\\n\\t\\n一瞬间我的...', '', 1720058251000, 1720058252000, '1', '0', '2', '0', 'https://sns-img-al.xhscdn.com/56933f1b-7539-e468-d433-cce1f0d2fafa', '新冠,中药', 'https://www.xiaohongshu.com/explore/6686018b000000001c026d4f', 1, 1, 0, 0, 1, 1, ''), (6, '56684f6550c4b411207ba204', '飘＊piao💋', 'https://sns-avatar-qc.xhscdn.com/avatar/1040g2jo30ujmg175587043rsd17mb8g4gentkf0', '云南', 1720058839737, 1720059297984, '6686011e000000000a004f94', 'normal', '家人们是怎么熬过来的？', '第一天吃了阿莫西林胶囊、蒲地蓝口服液\\n第二天吃了奥司他韦、盐酸左氧氟沙星片\\n第三天症状依旧、嗓子剧痛、喉咙充血、咳嗽痰带血丝…需要去医院吗\\n\\t\\n#吞刀片嗓 #新冠 #嗓子巨痛 #新型病毒 #', '', 1720058142000, 1720058143000, '0', '0', '5', '0', 'https://sns-img-bd.xhscdn.com/4e9459c5-ee7e-591d-418b-8ef21624a231', '吞刀片嗓,新冠,嗓子巨痛,新型病毒', 'https://www.xiaohongshu.com/explore/6686011e000000000a004f94', 1, 0, 0, 1, 1, 1, ''), (7, '5ff48a880000000001009b2e', 'ada', 'https://sns-avatar-qc.xhscdn.com/avatar/5ff48a880000000001009b2e.jpg', '广东', 1720058839742, 1720059297989, '6686000b000000001e011cb3', 'normal', '有谁还记得这个东西', '#疫情下的生活 #新冠', '', 1720057867000, 1720057867000, '0', '0', '0', '0', 'https://sns-img-al.xhscdn.com/fc2ed5a3-85f2-5187-6904-ff55bba56520', '疫情下的生活,新冠', 'https://www.xiaohongshu.com/explore/6686000b000000001e011cb3', 1, 1, 0, 1, 0, 0, ''), (8, '58aa256882ec39096904d305', '星星偷走睡意✨', 'https://sns-avatar-qc.xhscdn.com/avatar/62be6a71aff885dcf52703f4.jpg', '黑龙江', 1720058839748, 1720059297993, '6685fc6a000000001f005300', 'normal', '新冠后遗症吗？', '一直睡一直困🥱#新冠肺炎后遗症', '', 1720056938000, 1720056939000, '1', '0', '1', '0', 'https://sns-img-qc.xhscdn.com/7cc92958-95de-bc25-0652-2045a9d650fa', '新冠肺炎后遗症', 'https://www.xiaohongshu.com/explore/6685fc6a000000001f005300', 1, 1, 0, 1, 1, 1, ''), (9, '602f776e000000000101d5ec', '小白', 'https://sns-avatar-qc.xhscdn.com/avatar/664f0899523735166645cd6c.jpg', '山东', 1720058839753, 1720059297999, '6685f8ee000000000a027248', 'normal', '众多病号中的我#记得要给生活加点甜哦 ，我是个活泼却有距离感且坚强的人，本具很好的先天之气，自从2013年开始流浪漂泊的随心所欲过旅行生活后，常年出国和藏地的学习流浪，到了2022年一场高原佛学院新冠，彻底把我拉入了医院长跑队伍中，终于彻底的休息，瘫三月，轮椅两月，严重肌肉萎缩，哈哈哈，我知道以后会看着受罪的日子，我依然会笑笑夸自己很酷，因为我一直知道我会生一场大病，只是他来了而已，感恩相遇啊，我的疾病朋友，请爱上你的疾病并拥抱他吧，因为这就是你的态度，每个人都有得选，这是我选的而已#新冠', '众多病号中的我#记得要给生活加点甜哦 ，我是个活泼却有距离感且坚强的人，本具很好的先天之气，自从2013年开始流浪漂泊的随心所欲过旅行生活后，常年出国和藏地的学习流浪，到了2022年一场高原佛学院新冠，彻底把我拉入了医院长跑队伍中，终于彻底的休息，瘫三月，轮椅两月，严重肌肉萎缩，哈哈哈，我知道以后会看着受罪的日子，我依然会笑笑夸自己很酷，因为我一直知道我会生一场大病，只是他来了而已，感恩相遇啊，我的疾病朋友，请爱上你的疾病并拥抱他吧，因为这就是你的态度，每个人都有得选，这是我选的而已#新冠', '', 1720056046000, 1720056047000, '1', '0', '2', '0', 'https://sns-img-bd.xhscdn.com/40fb2c94-ed8f-9a3c-48a4-ab366500c604', '记得要给生活加点甜哦,新冠', 'https://www.xiaohongshu.com/explore/6685f8ee000000000a027248', 1, 1, 0, 1, 1, 1, ''), (10, '6332d634000000002303f575', 'awuge', 'https://sns-avatar-qc.xhscdn.com/avatar/6332d634000000002303f575.jpg', '上海', 1720058839758, 1720059298003, '6685f88c000000001c0246ca', 'normal', '#新冠后遗症，症状全✓[哭惹R][哭惹R][哭惹R]', '#新冠后遗症，症状全✓[哭惹R][哭惹R][哭惹R]', '', 1720055948000, 1720055948000, '1', '0', '0', '1', 'https://sns-img-qc.xhscdn.com/6e4062ae-0833-3d32-a578-df701f764fe9', '新冠后遗症', 'https://www.xiaohongshu.com/explore/6685f88c000000001c0246ca', 1, 1, 0, 1, 1, 1, '')]\n"
     ]
    }
   ],
   "source": [
    "db = SQLDatabase(engine=engine)\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "print(db.run(\"SELECT * FROM xhs_note LIMIT 10;\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5a3c3-707c-4331-9f5f-0cb4919763dd",
   "metadata": {},
   "source": [
    "And create a [SQL agent](/docs/tutorials/sql_qa) to interact with it:\n",
    "\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e868a586-4f4e-4b1d-ab11-fae1271dd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd92649-b178-47bd-b2b7-d5d4e14b3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "\n",
    "agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aefe929-5e39-4ed1-b135-aaf88edce2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mXhsNotes, bilibili_video, bilibili_video_comment, douyin_aweme, douyin_aweme_comment, dy_creator, kuaishou_video, kuaishou_video_comment, weibo_note, weibo_note_comment, xhs_creator, xhs_note, xhs_note_comment\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'xhs_note'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE xhs_note (\n",
      "\tid INTEGER NOT NULL COMMENT '自增ID' AUTO_INCREMENT, \n",
      "\tuser_id VARCHAR(64) NOT NULL COMMENT '用户ID', \n",
      "\tnickname VARCHAR(64) COMMENT '用户昵称', \n",
      "\tavatar VARCHAR(255) COMMENT '用户头像地址', \n",
      "\tip_location VARCHAR(255) COMMENT '评论时的IP地址', \n",
      "\tadd_ts BIGINT NOT NULL COMMENT '记录添加时间戳', \n",
      "\tlast_modify_ts BIGINT NOT NULL COMMENT '记录最后修改时间戳', \n",
      "\tnote_id VARCHAR(64) NOT NULL COMMENT '笔记ID', \n",
      "\ttype VARCHAR(16) COMMENT '笔记类型(normal | video)', \n",
      "\ttitle VARCHAR(255) COMMENT '笔记标题', \n",
      "\t`desc` LONGTEXT COMMENT '笔记描述', \n",
      "\tvideo_url LONGTEXT COMMENT '视频地址', \n",
      "\ttime BIGINT NOT NULL COMMENT '笔记发布时间戳', \n",
      "\tlast_update_time BIGINT NOT NULL COMMENT '笔记最后更新时间戳', \n",
      "\tliked_count VARCHAR(16) COMMENT '笔记点赞数', \n",
      "\tcollected_count VARCHAR(16) COMMENT '笔记收藏数', \n",
      "\tcomment_count VARCHAR(16) COMMENT '笔记评论数', \n",
      "\tshare_count VARCHAR(16) COMMENT '笔记分享数', \n",
      "\timage_list LONGTEXT COMMENT '笔记封面图片列表', \n",
      "\ttag_list LONGTEXT COMMENT '标签列表', \n",
      "\tnote_url VARCHAR(255) COMMENT '笔记详情页的URL', \n",
      "\tis_processed_llm TINYINT(1) COMMENT '是否已处理 LLM' DEFAULT '0', \n",
      "\tis_about_covid TINYINT(1) COMMENT '是否关于 COVID' DEFAULT '0', \n",
      "\tis_about_fever TINYINT(1) COMMENT '是否关于发烧' DEFAULT '0', \n",
      "\tis_about_virus TINYINT(1) COMMENT '是否关于病毒' DEFAULT '0', \n",
      "\tis_about_sick TINYINT(1) COMMENT '是否关于生病' DEFAULT '0', \n",
      "\tis_sick_recent TINYINT(1) COMMENT '是否最近生病' DEFAULT '0', \n",
      "\tsource_keyword VARCHAR(255) COMMENT '搜索来源关键字' DEFAULT '', \n",
      "\tPRIMARY KEY (id)\n",
      ")DEFAULT CHARSET=utf8mb4 ENGINE=InnoDB COLLATE utf8mb4_0900_ai_ci COMMENT='小红书笔记'\n",
      "\n",
      "/*\n",
      "3 rows from xhs_note table:\n",
      "id\tuser_id\tnickname\tavatar\tip_location\tadd_ts\tlast_modify_ts\tnote_id\ttype\ttitle\tdesc\tvideo_url\ttime\tlast_update_time\tliked_count\tcollected_count\tcomment_count\tshare_count\timage_list\ttag_list\tnote_url\tis_processed_llm\tis_about_covid\tis_about_fever\tis_about_virus\tis_about_sick\tis_sick_recent\tsource_keyword\n",
      "1\t59532abe5e87e746a3d3e49f\t酒酿宝\thttps://sns-avatar-qc.xhscdn.com/avatar/6640b4ec239fb0f0db0b2af4.jpg\t中国香港\t1720058839710\t1720059297964\t6685fed1000000000a0264bc\tnormal\t🇭🇰新冠首阳 五天转阴\t躲过了前两年的新冠高峰期，没想到去完高雄旅游完回来居然中招了，果然新冠不会漏掉每一个人🤦🏻‍♀️\n",
      "\t\n",
      "说说我这五天的一个症状吧：\n",
      "第一天6/27: 起床之后全身乏力，头痛炸裂，以为是旅游回来累的，晚上\t\t1720057553000\t1720058733000\t2\t0\t2\t0\thttps://sns-img-al.xhscdn.com/1040g2sg314qhj6f4hc7049kvgqlbtp4vbv8q408,https://sns-img-al.xhscdn.com\t新冠,抗疫日记,香港居家隔离,covid19\thttps://www.xiaohongshu.com/explore/6685fed1000000000a0264bc\t1\t1\t1\t1\t1\t1\t\n",
      "2\t62fe31ce000000000f005d1b\taquarius重生版\thttps://sns-avatar-qc.xhscdn.com/avatar/1040g2jo30vfkq54hm26g5onu6773on8r22tcfn8\t江苏\t1720058839717\t1720059297970\t6686035b000000000a024e84\tnormal\tSA55究竟什么情况？\t有没有大神了解的？就这么无了？[惊恐R][惊恐R][惊恐R][哭惹R][哭惹R][哭惹R]\n",
      "#新冠 #抗体 #新冠抗体\t\t1720058715000\t1720058715000\t0\t0\t3\t0\thttps://sns-img-qc.xhscdn.com/d02653aa-0635-d5c0-6abb-334ecb3e0e8d\t新冠,抗体,新冠抗体\thttps://www.xiaohongshu.com/explore/6686035b000000000a024e84\t1\t1\t0\t1\t1\t1\t\n",
      "3\t6667007500000000030313db\t没有作业的星球\thttps://sns-avatar-qc.xhscdn.com/avatar/1040g2jo314oqjb21h4005pj701qgu4urc9md17o\t美国\t1720058839723\t1720059297958\t66860307000000001e011928\tnormal\t数据分析新冠对美国人出行的影响\t2022年计算社会科学国际会议 (IC2S2) 举办了一场数据分析比赛，比赛时长一天。组委会提供了多种类型的数据，参赛者可以选择研究任何问题。我选择分析美国匹兹堡市疫情期间不同街道人们的出行数据。数据\t\t1720058631000\t1720058889000\t1\t0\t0\t0\thttps://sns-img-hw.xhscdn.com/07f4a5b2-e106-8405-d5f5-c53e0eba7565,https://sns-img-hw.xhscdn.com/f2d\t计算社会科学,新冠,疫情,旅游,美国,数据分析\thttps://www.xiaohongshu.com/explore/66860307000000001e011928\t1\t1\t0\t1\t0\t0\t\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `{'query': \"SELECT COUNT(*) AS num_notes FROM xhs_note WHERE `desc` LIKE '%旅游%'\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[(358,)]\u001b[0m\u001b[32;1m\u001b[1;3mThere are 358 notes in the xhs_note table with the content '旅游'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"how many notes with content '旅游' are there in xhs_note\",\n",
       " 'output': \"There are 358 notes in the xhs_note table with the content '旅游'.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how many notes with content '旅游' are there in xhs_note\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1eb128-842b-4018-87ab-bb269147f6ec",
   "metadata": {},
   "source": [
    "This approach easily generalizes to multiple CSVs, since we can just load each of them into our database as its own table. See the [Multiple CSVs](/docs/how_to/sql_csv#multiple-csvs) section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f2d91-2377-49dd-97a3-19d48a750715",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Instead of SQL we can also use data analysis libraries like pandas and the code generating abilities of LLMs to interact with CSV data. Again, **this approach is not fit for production use cases unless you have extensive safeguards in place**. For this reason, our code-execution utilities and constructors live in the `langchain-experimental` package.\n",
    "\n",
    "### Chain\n",
    "\n",
    "Most LLMs have been trained on enough pandas Python code that they can generate it just by being asked to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c84b27-9367-4c58-8a88-ade1fbf6683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "correlation = df['Age'].corr(df['Fare'])\n",
      "correlation\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm.invoke(\n",
    "    \"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else.\"\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e84003-5c39-496b-afa7-eaa50a01b7bb",
   "metadata": {},
   "source": [
    "We can combine this ability with a Python-executing tool to create a simple data analysis chain. We'll first want to load our CSV table as a dataframe, and give the tool access to this dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16abe312-b1a3-413f-bb9a-0e613d1e550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.30542018038331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_experimental.tools import PythonAstREPLTool\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "tool = PythonAstREPLTool(locals={\"df\": df})\n",
    "tool.invoke(\"df['Fare'].mean()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2e7c-6ea8-4674-98eb-a43c69f5c19d",
   "metadata": {},
   "source": [
    "To help enforce proper use of our Python tool, we'll using [tool calling](/docs/how_to/tool_calling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a9c8ec-1d06-4870-a584-b8d7b6c6ddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7exrNGUOLnoWgIcKnbKMgwrg', 'function': {'arguments': '{\"query\":\"df[[\\'Age\\', \\'Fare\\']].corr()\"}', 'name': 'python_repl_ast'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 125, 'total_tokens': 139, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-710466e6-f252-4a3c-ad96-96143a6f0ed3-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': \"df[['Age', 'Fare']].corr()\"}, 'id': 'call_7exrNGUOLnoWgIcKnbKMgwrg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 125, 'output_tokens': 14, 'total_tokens': 139, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)\n",
    "response = llm_with_tools.invoke(\n",
    "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e4015c-236d-42d7-ba8f-16052fa4f405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'python_repl_ast',\n",
       "  'args': {'query': \"df[['Age', 'Fare']].corr()\"},\n",
       "  'id': 'call_7exrNGUOLnoWgIcKnbKMgwrg',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec46fb-7296-443c-9e97-cfa9045ff21d",
   "metadata": {},
   "source": [
    "We'll add a tools output parser to extract the function call as a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476128f2-aa61-47f5-a371-dcff7b391d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"df[['Age', 'Fare']].corr()\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
    "\n",
    "parser = JsonOutputKeyToolsParser(key_name=tool.name, first_tool_only=True)\n",
    "(llm_with_tools | parser).invoke(\n",
    "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59362ea0-cc5a-4841-b87c-51d6a87d5810",
   "metadata": {},
   "source": [
    "And combine with a prompt so that we can just specify a question without needing to specify the dataframe info every invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e87a820-e4ce-417e-b580-043fb2d5c8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"df[['Age', 'Fare']].corr()\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = f\"\"\"You have access to a pandas dataframe `df`. \\\n",
    "Here is the output of `df.head().to_markdown()`:\n",
    "\n",
    "```\n",
    "{df.head().to_markdown()}\n",
    "```\n",
    "\n",
    "Given a user question, write the Python code to answer it. \\\n",
    "Return ONLY the valid Python code and nothing else. \\\n",
    "Don't assume you have access to any libraries other than built-in Python ones and pandas.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])\n",
    "code_chain = prompt | llm_with_tools | parser\n",
    "code_chain.invoke({\"question\": \"What's the correlation between age and fare\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63989e47-c0af-409e-9766-83c3fe6d69bb",
   "metadata": {},
   "source": [
    "And lastly we'll add our Python tool so that the generated code is actually executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e56a891-4c3f-4e5a-a5ee-3973112ffeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11232863699941621"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm_with_tools | parser | tool\n",
    "chain.invoke({\"question\": \"What's the correlation between age and fare\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb12764-4a90-4e84-88b4-a25949084ea2",
   "metadata": {},
   "source": [
    "And just like that we have a simple data analysis chain. We can take a peak at the intermediate steps by looking at the LangSmith trace: https://smith.langchain.com/public/b1309290-7212-49b7-bde2-75b39a32b49a/r\n",
    "\n",
    "We could add an additional LLM call at the end to generate a conversational response, so that we're not just responding with the tool output. For this we'll want to add a chat history `MessagesPlaceholder` to our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe3818d-0657-4729-ac46-ab5d4860d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system = f\"\"\"You have access to a pandas dataframe `df`. \\\n",
    "Here is the output of `df.head().to_markdown()`:\n",
    "\n",
    "```\n",
    "{df.head().to_markdown()}\n",
    "```\n",
    "\n",
    "Given a user question, write the Python code to answer it. \\\n",
    "Don't assume you have access to any libraries other than built-in Python ones and pandas.\n",
    "Respond directly to the question once you have enough information to answer it.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system,\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # This MessagesPlaceholder allows us to optionally append an arbitrary number of messages\n",
    "        # at the end of the prompt using the 'chat_history' arg.\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _get_chat_history(x: dict) -> list:\n",
    "    \"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"\n",
    "    ai_msg = x[\"ai_msg\"]\n",
    "    tool_call_id = x[\"ai_msg\"].additional_kwargs[\"tool_calls\"][0][\"id\"]\n",
    "    tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x[\"tool_output\"]))\n",
    "    return [ai_msg, tool_msg]\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)\n",
    "    .assign(tool_output=itemgetter(\"ai_msg\") | parser | tool)\n",
    "    .assign(chat_history=_get_chat_history)\n",
    "    .assign(response=prompt | llm | StrOutputParser())\n",
    "    .pick([\"tool_output\", \"response\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6e98ec-52f1-4ffd-9ea8-bacedfa29f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_output':            Age      Fare\n",
       " Age   1.000000  0.112329\n",
       " Fare  0.112329  1.000000,\n",
       " 'response': 'The correlation between age and fare is approximately 0.1123. This indicates a very weak positive correlation between these two variables.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What's the correlation between age and fare\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a5a91-c6d2-4a40-9b9f-eb38f78c9d22",
   "metadata": {},
   "source": [
    "Here's the LangSmith trace for this run: https://smith.langchain.com/public/14e38d70-45b1-4b81-8477-9fd2b7c07ea6/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24b4f4-abbf-4891-b200-814eb9c35bec",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "For complex questions it can be helpful for an LLM to be able to iteratively execute code while maintaining the inputs and outputs of its previous executions. This is where Agents come into play. They allow an LLM to decide how many times a tool needs to be invoked and keep track of the executions it's made so far. The [create_pandas_dataframe_agent](https://python.langchain.com/api_reference/experimental/agents/langchain_experimental.agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent.html) is a built-in agent that makes it easy to work with dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ea904e-795f-411b-bef8-6484dbb6e35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Please read the security notice in the doc-string of this function. You must opt-in to use this functionality by setting allow_dangerous_code=True.For general security guidelines, please see: https://python.langchain.com/v0.2/docs/security/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_pandas_dataframe_agent\n\u001b[0;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pandas_dataframe_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai-tools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m agent\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      5\u001b[0m     {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the correlation between age and fare? is that greater than the correlation between fare and survival?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langchain_experimental/agents/agent_toolkits/pandas/base.py:249\u001b[0m, in \u001b[0;36mcreate_pandas_dataframe_agent\u001b[0;34m(llm, df, agent_type, callback_manager, prefix, suffix, input_variables, verbose, return_intermediate_steps, max_iterations, max_execution_time, early_stopping_method, agent_executor_kwargs, include_df_in_prompt, number_of_head_rows, extra_tools, engine, allow_dangerous_code, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a Pandas agent from an LLM and dataframe(s).\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03mSecurity Notice:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_code:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis agent relies on access to a python repl tool which can execute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marbitrary code. This can be dangerous and requires a specially sandboxed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment to be safely used. Please read the security notice in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc-string of this function. You must opt-in to use this functionality \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting allow_dangerous_code=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor general security guidelines, please see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/v0.2/docs/security/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: This agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Please read the security notice in the doc-string of this function. You must opt-in to use this functionality by setting allow_dangerous_code=True.For general security guidelines, please see: https://python.langchain.com/v0.2/docs/security/"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm, df, agent_type=\"openai-tools\", verbose=True, allow_dangerous_code=True\n",
    ")\n",
    "agent.invoke(\n",
    "    {\n",
    "        \"input\": \"What's the correlation between age and fare? is that greater than the correlation between fare and survival?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65322f3-b13c-4949-82b2-4517b9a0859d",
   "metadata": {},
   "source": [
    "Here's the LangSmith trace for this run: https://smith.langchain.com/public/6a86aee2-4f22-474a-9264-bd4c7283e665/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68492261-faef-47e7-8009-e20ef1420d5a",
   "metadata": {},
   "source": [
    "### Multiple CSVs {#multiple-csvs}\n",
    "\n",
    "To handle multiple CSVs (or dataframes) we just need to pass multiple dataframes to our Python tool. Our `create_pandas_dataframe_agent` constructor can do this out of the box, we can pass in a list of dataframes instead of just one. If we're constructing a chain ourselves, we can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a70e1b-d3ee-4fa6-a4a0-d2e5005e6c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14384991262954416"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = df[[\"Age\", \"Fare\"]]\n",
    "df_2 = df[[\"Fare\", \"Survived\"]]\n",
    "\n",
    "tool = PythonAstREPLTool(locals={\"df_1\": df_1, \"df_2\": df_2})\n",
    "llm_with_tool = llm.bind_tools(tools=[tool], tool_choice=tool.name)\n",
    "df_template = \"\"\"```python\n",
    "{df_name}.head().to_markdown()\n",
    ">>> {df_head}\n",
    "```\"\"\"\n",
    "df_context = \"\\n\\n\".join(\n",
    "    df_template.format(df_head=_df.head().to_markdown(), df_name=df_name)\n",
    "    for _df, df_name in [(df_1, \"df_1\"), (df_2, \"df_2\")]\n",
    ")\n",
    "\n",
    "system = f\"\"\"You have access to a number of pandas dataframes. \\\n",
    "Here is a sample of rows from each dataframe and the python code that was used to generate the sample:\n",
    "\n",
    "{df_context}\n",
    "\n",
    "Given a user question about the dataframes, write the Python code to answer it. \\\n",
    "Don't assume you have access to any libraries other than built-in Python ones and pandas. \\\n",
    "Make sure to refer only to the variables mentioned above.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])\n",
    "\n",
    "chain = prompt | llm_with_tool | parser | tool\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"return the difference in the correlation between age and fare and the correlation between fare and survival\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043363f-4ab1-41de-9318-c556e4ae66bc",
   "metadata": {},
   "source": [
    "Here's the LangSmith trace for this run: https://smith.langchain.com/public/cc2a7d7f-7c5a-4e77-a10c-7b5420fcd07f/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2256d09-23c2-4e52-bfc6-c84eba538586",
   "metadata": {},
   "source": [
    "### Sandboxed code execution\n",
    "\n",
    "There are a number of tools like [E2B](/docs/integrations/tools/e2b_data_analysis) and [Bearly](/docs/integrations/tools/bearly) that provide sandboxed environments for Python code execution, to allow for safer code-executing chains and agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728e791-f114-41e6-aa12-0436fdeeedae",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "For more advanced data analysis applications we recommend checking out:\n",
    "\n",
    "* [SQL tutorial](/docs/tutorials/sql_qa): Many of the challenges of working with SQL db's and CSV's are generic to any structured data type, so it's useful to read the SQL techniques even if you're using Pandas for CSV data analysis.\n",
    "* [Tool use](/docs/how_to/tool_calling): Guides on general best practices when working with chains and agents that invoke tools\n",
    "* [Agents](/docs/tutorials/agents): Understand the fundamentals of building LLM agents.\n",
    "* Integrations: Sandboxed envs like [E2B](/docs/integrations/tools/e2b_data_analysis) and [Bearly](/docs/integrations/tools/bearly), utilities like [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase), related agents like [Spark DataFrame agent](/docs/integrations/tools/spark_sql)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
